{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Feb 11 11:20:27 2021\n",
    "\n",
    "@author: erce\n",
    "\"\"\"\n",
    "import pathlib\n",
    "# Get the current path\n",
    "currentPath = pathlib.Path().absolute()\n",
    "import sys\n",
    "sys.path.insert(0, str(currentPath) + '/..')\n",
    "import pysansa\n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pysansa.rdf.rdf import Rdf\n",
    "from pysansa.query.query import Query\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating SparkConfig, SparkContext and SparkSession\n",
    "SparkContext uses our SANSA-Stack jar with dependencies included\n",
    "\"\"\"\n",
    "# Spark Session and Config\n",
    "conf = SparkConf().set(\"spark.jars\", str(currentPath) + \"../../pysansa/myjars/SANSA_all_dep_NO_spark.jar\") \n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# Spark object\n",
    "spark = sc._jvm.org.apache.spark.sql.SparkSession.builder().master(\"local\") \\\n",
    "                        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "                        .config(\"spark.sql.legacy.allowUntypedScalaUDF\", \"true\").appName(\"SansaRDF\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating Rdf Object from SANSA-Stack RDF Python Wrapper\n",
    "\"\"\"\n",
    "# Rdf object\n",
    "rdf = Rdf(sc)\n",
    "# Initialize Rdf Reader\n",
    "rdfReader = rdf.initializeRdfReader(spark)\n",
    "# Read triples from the given path\n",
    "triples = rdf.readTriples(rdfReader, path = 'file:///' + str(currentPath) + '../../data/rdf.nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of triples: 106\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running some examples with RDF Python Wrapper\n",
    "\"\"\"\n",
    "# Count triples from the object\n",
    "size = rdf.count()\n",
    "# Print size of triples\n",
    "print(\"Size of triples: \" + str(size))\n",
    "# Get triples as array\n",
    "triples = rdf.getTriples(30)\n",
    "# Get triples from Rdf object\n",
    "triples = rdf.triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Running some examples with Query Python Wrapper\n",
    "\"\"\"\n",
    "# Create Query object\n",
    "query = Query(sc)\n",
    "# Set and create Sparqlify executor object\n",
    "query.setTriplesToSparqlifyExecutor(triples)\n",
    "# An example query\n",
    "query1 = \"SELECT * WHERE {?s ?p ?o} LIMIT 106\"\n",
    "# Run the query and return the result object\n",
    "result = query.runQueryOnSparqlify(query1)\n",
    "# Print the result to the console as a table\n",
    "query.show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$anonfun$checkpoint$1', '$anonfun$checkpoint$2', '$anonfun$collect$1', '$anonfun$collectAsArrowToPython$1', '$anonfun$collectAsArrowToPython$1$adapted', '$anonfun$collectAsArrowToPython$2', '$anonfun$collectAsArrowToPython$2$adapted', '$anonfun$collectAsArrowToPython$3', '$anonfun$collectAsArrowToPython$3$adapted', '$anonfun$collectAsArrowToPython$4', '$anonfun$collectAsArrowToPython$5', '$anonfun$collectAsArrowToPython$6', '$anonfun$collectAsArrowToPython$7', '$anonfun$collectAsArrowToPython$8', '$anonfun$collectAsArrowToPython$9', '$anonfun$collectAsArrowToPython$9$adapted', '$anonfun$collectAsArrowToR$1', '$anonfun$collectAsArrowToR$1$adapted', '$anonfun$collectAsArrowToR$2', '$anonfun$collectAsArrowToR$2$adapted', '$anonfun$collectAsArrowToR$3', '$anonfun$collectAsArrowToR$4', '$anonfun$collectAsArrowToR$4$adapted', '$anonfun$collectAsList$1', '$anonfun$collectToPython$1', '$anonfun$collectToPython$2', '$anonfun$columns$1', '$anonfun$count$1', '$anonfun$count$1$adapted', '$anonfun$cube$1', '$anonfun$cube$2', '$anonfun$drop$1', '$anonfun$drop$1$adapted', '$anonfun$drop$2', '$anonfun$drop$2$adapted', '$anonfun$drop$3', '$anonfun$drop$4', '$anonfun$drop$5', '$anonfun$drop$5$adapted', '$anonfun$drop$6', '$anonfun$dropDuplicates$1', '$anonfun$dropDuplicates$2', '$anonfun$dropDuplicates$2$adapted', '$anonfun$dtypes$1', '$anonfun$explain$1', '$anonfun$explode$1', '$anonfun$explode$2', '$anonfun$explode$3', '$anonfun$explode$4', '$anonfun$explode$5', '$anonfun$flatMap$1', '$anonfun$flatMap$2', '$anonfun$foreach$1', '$anonfun$foreach$2', '$anonfun$foreach$2$adapted', '$anonfun$foreachPartition$1', '$anonfun$foreachPartition$2', '$anonfun$foreachPartition$2$adapted', '$anonfun$getRows$1', '$anonfun$getRows$2', '$anonfun$getRows$3', '$anonfun$getRows$4', '$anonfun$getRows$4$adapted', '$anonfun$getRowsToPython$1', '$anonfun$getRowsToPython$2', '$anonfun$groupBy$1', '$anonfun$groupBy$2', '$anonfun$groupByKey$1', '$anonfun$head$1', '$anonfun$inputFiles$1', '$anonfun$inputFiles$1$adapted', '$anonfun$isEmpty$1', '$anonfun$isEmpty$1$adapted', '$anonfun$javaToPython$1', '$anonfun$join$1', '$anonfun$logicalPlan$1', '$anonfun$logicalPlan$2', '$anonfun$logicalPlan$2$adapted', '$anonfun$logicalPlan$3', '$anonfun$mapPartitions$1', '$anonfun$numericColumns$1', '$anonfun$numericColumns$1$adapted', '$anonfun$numericColumns$2', '$anonfun$observe$1', '$anonfun$randomSplit$1', '$anonfun$randomSplit$2', '$anonfun$randomSplit$3', '$anonfun$randomSplit$4', '$anonfun$randomSplit$4$adapted', '$anonfun$randomSplit$5', '$anonfun$randomSplit$6', '$anonfun$randomSplit$7', '$anonfun$randomSplit$8', '$anonfun$rdd$1', '$anonfun$rdd$2', '$anonfun$reduce$1', '$anonfun$reduce$2', '$anonfun$repartition$1', '$anonfun$repartition$1$adapted', '$anonfun$repartition$2', '$anonfun$repartitionByRange$1', '$anonfun$repartitionByRange$2', '$anonfun$resolve$1', '$anonfun$resolve$2', '$anonfun$resolve$2$adapted', '$anonfun$rollup$1', '$anonfun$rollup$2', '$anonfun$schema$1', '$anonfun$select$1', '$anonfun$select$2', '$anonfun$select$2$adapted', '$anonfun$select$3', '$anonfun$select$4', '$anonfun$select$5', '$anonfun$selectExpr$1', '$anonfun$selectUntyped$1', '$anonfun$selectUntyped$2', '$anonfun$showString$1', '$anonfun$showString$10', '$anonfun$showString$10$adapted', '$anonfun$showString$11', '$anonfun$showString$11$adapted', '$anonfun$showString$12', '$anonfun$showString$13', '$anonfun$showString$2', '$anonfun$showString$2$adapted', '$anonfun$showString$3', '$anonfun$showString$3$adapted', '$anonfun$showString$4', '$anonfun$showString$4$adapted', '$anonfun$showString$5', '$anonfun$showString$6', '$anonfun$showString$7', '$anonfun$showString$7$adapted', '$anonfun$showString$8', '$anonfun$showString$9', '$anonfun$showString$9$adapted', '$anonfun$sort$1', '$anonfun$sortInternal$1', '$anonfun$sortWithinPartitions$1', '$anonfun$storageLevel$1', '$anonfun$storageLevel$2', '$anonfun$tail$1', '$anonfun$tailToPython$1', '$anonfun$tailToPython$2', '$anonfun$toArrowBatchRdd$1', '$anonfun$toDF$1', '$anonfun$toDF$2', '$anonfun$toDF$3', '$anonfun$toJSON$1', '$anonfun$toLocalIterator$1', '$anonfun$toPythonIterator$1', '$anonfun$toString$1', '$anonfun$unionByName$1', '$anonfun$unionByName$2', '$anonfun$unionByName$3', '$anonfun$unionByName$4', '$anonfun$unionByName$4$adapted', '$anonfun$unionByName$5', '$anonfun$unionByName$6', '$anonfun$withAction$1', '$anonfun$withColumnRenamed$1', '$anonfun$withColumnRenamed$1$adapted', '$anonfun$withColumnRenamed$2', '$anonfun$withColumns$1', '$anonfun$withColumns$2', '$anonfun$withColumns$3', '$anonfun$withColumns$3$adapted', '$anonfun$withColumns$4', '$anonfun$withColumns$4$adapted', '$anonfun$withColumns$5', '$anonfun$withColumns$5$adapted', '$anonfun$withColumns$6', '$anonfun$withColumns$7', '$anonfun$withColumns$8', '$anonfun$withNewRDDExecutionId$1', '$anonfun$withWatermark$1', 'COL_POS_KEY', 'DATASET_ID_KEY', 'DATASET_ID_TAG', 'agg', 'alias', 'apply', 'as', 'cache', 'checkpoint', 'coalesce', 'col', 'colRegex', 'collect', 'collectAsArrowToPython', 'collectAsArrowToR', 'collectAsList', 'collectToPython', 'columns', 'count', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'cube', 'curId', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dtypes', 'encoder', 'equals', 'except', 'exceptAll', 'explain', 'explode', 'exprEnc', 'filter', 'first', 'flatMap', 'foreach', 'foreachPartition', 'getClass', 'getRows', 'getRowsToPython', 'groupBy', 'groupByKey', 'hashCode', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'javaRDD', 'javaToPython', 'join', 'joinWith', 'limit', 'localCheckpoint', 'logicalPlan', 'map', 'mapInPandas', 'mapPartitions', 'mapPartitionsInR', 'na', 'notify', 'notifyAll', 'numericColumns', 'observe', 'ofRows', 'orderBy', 'org$apache$spark$sql$Dataset$$classTag', 'org$apache$spark$sql$Dataset$$id', 'org$apache$spark$sql$Dataset$$resolvedEnc', 'org$apache$spark$sql$Dataset$$withPlan', 'persist', 'printSchema', 'queryExecution', 'randomSplit', 'randomSplitAsList', 'rdd', 'reduce', 'registerTempTable', 'repartition', 'repartitionByRange', 'resolve', 'rollup', 'sample', 'schema', 'select', 'selectExpr', 'selectUntyped', 'show', 'showString', 'showString$default$2', 'showString$default$3', 'sort', 'sortWithinPartitions', 'sparkSession', 'sqlContext', 'stat', 'storageLevel', 'summary', 'tail', 'tailToPython', 'take', 'takeAsList', 'toArrowBatchRdd', 'toDF', 'toJSON', 'toJavaRDD', 'toLocalIterator', 'toPythonIterator', 'toPythonIterator$default$1', 'toString', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'wait', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
     ]
    }
   ],
   "source": [
    "# Print possible usable functions of the result object\n",
    "query.printAttributes(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Element: [http://commons.dbpedia.org/property/width,null,null,http://commons.dbpedia.org/resource/Category:People,null,100.0,null,null]\n",
      "\n",
      "2. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:Buswachten.jpg,null,null,null,2004-07-22]\n",
      "\n",
      "3. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:Groninger-museum.jpg,null,null,null,2004-08-26]\n",
      "\n",
      "4. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:StationAssen3.jpg,null,null,null,2004-07-22]\n",
      "\n",
      "5. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:De_Slegte,_Groningen.jpg,null,null,null,2004-08-26]\n",
      "\n",
      "6. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:Paddestoel_003.jpg,null,null,null,2004-08-20]\n",
      "\n",
      "7. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:BordUtrecht.jpg,null,null,null,2004-07-22]\n",
      "\n",
      "8. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:Paddestoel_002.jpg,null,null,null,2004-08-20]\n",
      "\n",
      "9. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:Groningen_003.jpg,null,null,null,2004-08-26]\n",
      "\n",
      "10. Element: [http://commons.dbpedia.org/property/date,null,null,http://commons.dbpedia.org/resource/File:StationAssen2.jpg,null,null,null,2004-07-22]\n"
     ]
    }
   ],
   "source": [
    "# Convert the result to DataFrame\n",
    "dfResult = query.convertToDataFrame(result)\n",
    "# Get 10 rows from the DataFrame\n",
    "dfResultArray = query.takeFromDataFrame(dfResult, 10)\n",
    "# Print the row array that was taken from the DataFrame\n",
    "query.printDF(dfResultArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"C_3\":\"http://commons.dbpedia.org/property/width\",\"C_4\":null,\"C_5\":null,\"C_10\":\"http://commons.dbpedia.org/resource/Category:People\",\"C_6\":null,\"C_7\":\"100.0\",\"C_8\":null,\"C_9\":null}\n",
      "JObject(List((C_3,JString(http://commons.dbpedia.org/property/width)), (C_4,JNull), (C_5,JNull), (C_10,JString(http://commons.dbpedia.org/resource/Category:People)), (C_6,JNull), (C_7,JString(100.0)), (C_8,JNull), (C_9,JNull)))\n",
      "StructType(StructField(C_3,StringType,false), StructField(C_4,StringType,true), StructField(C_5,StringType,true), StructField(C_10,StringType,true), StructField(C_6,StringType,true), StructField(C_7,StringType,true), StructField(C_8,StringType,true), StructField(C_9,DateType,true))\n",
      "{\n",
      "  \"C_3\" : \"http://commons.dbpedia.org/property/width\",\n",
      "  \"C_4\" : null,\n",
      "  \"C_5\" : null,\n",
      "  \"C_10\" : \"http://commons.dbpedia.org/resource/Category:People\",\n",
      "  \"C_6\" : null,\n",
      "  \"C_7\" : \"100.0\",\n",
      "  \"C_8\" : null,\n",
      "  \"C_9\" : null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to JSON \n",
    "json = result.toJSON()\n",
    "# Print JSON\n",
    "json.show()\n",
    "# Get the first JSON element\n",
    "first = json.first()\n",
    "# Get the 5 first element from the JSON\n",
    "taken = json.take(5)\n",
    "# Get the first element from the JSON Array\n",
    "taken[0]\n",
    "# Print the JSON array\n",
    "# print(dir(taken))\n",
    "# Get the first row of the DataFrame Array\n",
    "firstRow = query.getRow(dfResultArray, 0)\n",
    "# Get the first column of the first row\n",
    "firstColumnOfFirstRow = query.getColumn(firstRow, 0)\n",
    "# Get the third column of the first row\n",
    "firstColumnOfFirstRow = query.getColumn(firstRow, 2)\n",
    "# Get the fourth column of the first row\n",
    "firstColumnOfFirstRow = query.getColumn(firstRow, 3)\n",
    "# Get the second row of the DataFrame Array\n",
    "secondRow = query.getRow(dfResultArray, 1)\n",
    "# JSON of the second row\n",
    "jsonOfSecondRow = secondRow.json()\n",
    "# Print the JSON of the second row\n",
    "print(jsonOfSecondRow)\n",
    "# Print JSON Value\n",
    "print(secondRow.jsonValue())\n",
    "# Print Schema\n",
    "print(secondRow.schema())\n",
    "# Print pretty JSON\n",
    "print(secondRow.prettyJson())\n",
    "# Stop SparkContext to prevent overloading\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
